---
title: "ANR Report - Target registration error distribution"
author:
  - Guillaume POTIER
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction
Dans son papier Fitzpatrick montre :

$$E(TRE^{2}(r)) = E(FLE^{2}) \times \left(\frac{1}{N} + \frac{1}{3} \times \sum_{i = 1}^{3}{\frac{d_{i}^{2}}{f_{i}^{2}}}\right)$$

Nous voulons vérifier si ce résultat est compatible du point de vue de la régression linéaire.

# Developpement dimension 1
Soit le modèle de régression linéaire simple :

$$y = X \times \beta + \epsilon$$

avec $X = \begin{bmatrix}1 & x\end{bmatrix}$, $\beta = \begin{bmatrix}\beta_{0}\\\beta_{1}\end{bmatrix}$ et $\epsilon \sim \mathcal{N}(0, \sigma^{2})$. Nous pouvons estimer $\beta$ et $\epsilon$ par $\hat{\beta}$ et $\hat{\epsilon}$ obtenus par l'estimateur des moindres carrés. Soit un point $y_0$ suivant le modèle de régression linéaire :

$$y_0 = X_0 \times \beta + \epsilon_0$$

avec $X_0 = \begin{bmatrix}1 & x_0\end{bmatrix}$ et $\beta = \begin{bmatrix}\beta_{0}\\\beta_{1}\end{bmatrix}$. Connaissant $x_0$ nous pouvons calculer $\hat{y_0}$ une estimation de $y_0$ :

$$\hat{y_0} = X_0 \times \hat{\beta}$$

Par ailleurs puisque $y_0$ suit le modèle de régression nous avons :

$$\epsilon_0 = y_0 - X_0 \times \beta$$

Que nous pouvons approcher par :

$$\hat{\epsilon_0} = y_0 - X_0 \times \hat{\beta}$$

```{r fig.width = 8, fig.height = 6, fig.align = "center"}
x = runif(15, 5, 10)
b0 = 2
b1 = 5.31
y = x * b1 + b0
y.noisy = y + rnorm(length(y), 0, 10)
reg = lm(y.noisy ~ x)
x0 = runif(1, 5, 10)
y0 = x0 * b1 + b0 + rnorm(1, 0, 0)
x1 = runif(1, 5, 10)
y1 = x0 * b1 + b0 + rnorm(1, 0, 1)
plot(c(x), c(y.noisy), pch = 16, xlab = "x", ylab = "y", xlim = c(min(c(x, x0, x1)), max(c(x, x0, x1))), ylim = c(min(c(y.noisy, y0, y1)) - 15, max(c(y.noisy, y0, y1))), xaxt = 'n', yaxt = 'n')
abline(a = b0, b = b1, lwd = 2)
abline(reg, col = "dodgerblue", lwd = 2)
points(x0, y0, col = "red3", pch = 16)
segments(
    x0 = x0,
    y0 = y0,
    x1 = x0,
    y1 = reg$coefficients[2] * x0 + reg$coefficients[1],
    col = "red3",
    lwd = 2
)
points(x1, y1, col = "seagreen3", pch = 16)
segments(
    x0 = x1,
    y0 = y1,
    x1 = x1,
    y1 = reg$coefficients[2] * x1 + reg$coefficients[1],
    col = "seagreen3",
    lwd = 2
)
legend("bottomleft", ncol = 2, legend = c("true model", "point from true model + noise used for fitting", "fitted model", "point from true model not used for fitting", "point from true model + noise not used for fitting"), lty = c(1, NA, 1, NA, NA), lwd = c(2, NA, 2, NA, NA), col = c("black", "black", "dodgerblue", "red3", "seagreen3"), pch = c(NA, 16, NA, 16, 16), cex = 0.75, border = "top")
```

Sur la figure ci-dessus, la droite noire représente le modèle. Les points noirs représentent les points issus du modèle avec un bruit gaussien. La droite bleue est le modèle estimé par les moindres carrés à partir des points noirs. Le point rouge est un point issu du modèle sans bruit. Le point vert est un point issu du modèle avec un bruit gaussien. Le segment vert correspond à $\hat{\epsilon_0}$. Le TRE de Fitzpatrick est défini comme la distance entre un point sur l'image cible et sa prédiction à partir du point associé sur l'image source. Le TRE est donc donné par $\left\lvert\hat{\epsilon_0}\right\rvert$. Puisque $\hat{\epsilon_0} \sim \mathcal{N}(0, \sigma^{2})$, alors $\left\lvert\hat{\epsilon_0}\right\rvert$ suit une loi demi-normale de moyenne $\frac{\sigma\sqrt{2}}{\sqrt{\pi}}$ et de variance $\sigma^{2}\left(1 - \frac{2}{\pi}\right)$.

# Developpement dimension p
Soit le modèle de régression linéaire multivarié :

$$Y_{n \times p} = X_{n \times (p+1)} \times \beta_{(p+1) \times p} + \epsilon_{n \times p}$$

Avec $\epsilon \sim \mathcal{N}(0, \Sigma)$

Nous pouvons estimer $\beta$ et $\epsilon$ par $\hat{\beta} = (X^{'}X)^{-1}X^{'}Y$ et $\hat{\epsilon} = Y - X\hat{\beta}$ obtenus par l'estimateur des moindres carrés. Soit un point $y_0$ suivant le modèle de régression linéaire :

$$y_0 = X_0 \times \beta + \epsilon_0$$

Nous avons :

$$\hat{\epsilon_0} = y_0 - X_0 \times \hat{\beta}$$

Et :

$$\hat{\epsilon_0} \sim \mathcal{N}_{p}\left(0, \left(1 + X_0^{'}(Z^{'}Z)^{-1}X_0\right)\Sigma\right)$$

En reprenant le raisonnement développé en dimension 1, nous voulons connaître la distribution de $\|\hat{\epsilon_{0}}\|^{2}$.

$$\|\hat{\epsilon_0}\|^{2} = \sum_{i = 1}^{p}{\hat{\epsilon_{0i}}^{2}}$$

Puisque $\hat{\epsilon_{0}} \sim \mathcal{N}_{p}\left(0, \left(1 + X_0^{'}(Z^{'}Z)^{-1}X_0\right)\Sigma\right)$, alors $\forall i \in [1, p]^{\mathbb{N}}, \hat{\epsilon_{0i}} \sim \mathcal{N}(0, \sigma_{i}^{2})$ . Si $\Sigma = \sigma^{2}I_p$, alors les $\hat{\epsilon_{0i}}$ sont indépendants et identiquement distribués et :

$$\|\hat{\epsilon_0}\|^{2} \sim \sigma^{2}\left(1 + X_0^{'}(Z^{'}Z)^{-1}X_0\right)\mathcal{X}^{2}_p$$

Alors :

$$E(\|\hat{\epsilon_0}\|^{2}) = p\sigma^{2}\left(1 + X_0^{'}(Z^{'}Z)^{-1}X_0\right)$$

Et :

$$Var(\|\hat{\epsilon_0}\|^{2}) = 2p\sigma^{4}\left(1 + X_0^{'}(Z^{'}Z)^{-1}X_0\right)^{2}$$

De la même façon :

$$\|\hat{\epsilon_0}\| \sim \sqrt{\sigma^{2}\left(1 + X_0^{'}(Z^{'}Z)^{-1}X_0\right)}\mathcal{X}_p$$

Alors :

$$E(\|\hat{\epsilon_0}\|) = \sqrt{\sigma^{2}\left(1 + X_0^{'}(Z^{'}Z)^{-1}X_0\right)} \times \sqrt{2}\frac{\Gamma(\frac{p+1}{2})}{\Gamma(\frac{p}{2})}$$

Et :

$$Var(\|\hat{\epsilon_0}\|) = \sigma^{2}\left(1 + X_0^{'}(Z^{'}Z)^{-1}X_0\right) \times \left(p - \left(\sqrt{2}\frac{\Gamma(\frac{p+1}{2})}{\Gamma(\frac{p}{2})}\right)^{2}\right)$$

En dimension 1 nous avons trouvé que $|\hat{\epsilon_{0}}|$ suit une loi demi-normale. La loi demi-normale est équivalente à une loi du $\mathcal{X}$ à 1 degré de liberté. Le développement en dimension p reste donc compatible avec le développement en dimension 1.
